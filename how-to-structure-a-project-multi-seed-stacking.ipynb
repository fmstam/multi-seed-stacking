{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This notebook describes how to structure a project.\n\nDuring the competition there were many tasks to do, plan, research, test, tune, accept disappointment of failed tests, and be happy with a little improvement. \n\nTherefore, the workspace and the files of the project should be structured in a flexible way with less repeated code. In other words, to split code from data/configuration to save time and reduce errors/bugs.\n\nTherefore, let us first define the main entities in the project.\nThere are four main entities and I think these will be the same in all projects: **experiment**, **model**, **level**, and **stack**. These will be modeled by classes as described in this notebook.\n\n\nI would like to give credits to many kernels and websites among them:\n\n - Good introduction introduction about stacking: https://mlwave.com/kaggle-ensembling-guide/\n - Implementation of stacking and a nice discussion: https://www.kaggle.com/getting-started/18153#post103381\n - Stacking solution for a regression problem: https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n - Fine tuned XGboost params: https://www.kaggle.com/stevenrferrer/30-days-of-ml-optimized-xgboost-5folds\n - Stacking pipeline: https://www.kaggle.com/abhishek/competition-part-6-stacking\n - Multi-seeds stacking: https://www.kaggle.com/hungkhoi/1st-place-stacking-code\n \n \n*This is a draft work, and will be improved regularly.*\n","metadata":{}},{"cell_type":"code","source":"# Familiar imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\nimport os\nimport glob\nfrom datetime import datetime\nfrom pathlib import Path\n\n\n\n# helpers\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder, PowerTransformer, StandardScaler, \\\n                                  MinMaxScaler, RobustScaler, PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\nfrom sklearn.pipeline import make_pipeline\n\n# Models\nfrom sklearn.linear_model import ElasticNet, Lasso, LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# base\nfrom sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, clone\n\n# scoring\nfrom sklearn.metrics import mean_squared_error\n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:30:58.102696Z","iopub.execute_input":"2021-09-06T14:30:58.103176Z","iopub.status.idle":"2021-09-06T14:31:00.781408Z","shell.execute_reply.started":"2021-09-06T14:30:58.103048Z","shell.execute_reply":"2021-09-06T14:31:00.780643Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"# notebook options\npd.set_option(\"display.max_columns\", 100)\npath = \"../input/30-days-of-ml/\"\ntrain_file = \"train.csv\"\ntest_file = \"test.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:00.782735Z","iopub.execute_input":"2021-09-06T14:31:00.783217Z","iopub.status.idle":"2021-09-06T14:31:00.786860Z","shell.execute_reply.started":"2021-09-06T14:31:00.783188Z","shell.execute_reply":"2021-09-06T14:31:00.786156Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load the training data\ntrain = pd.read_csv(f'{path}{os.sep}{train_file}', index_col=0)\ntest = pd.read_csv(f'{path}{os.sep}{test_file}', index_col=0)\n\n# Preview the data\n# train.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:00.788617Z","iopub.execute_input":"2021-09-06T14:31:00.788911Z","iopub.status.idle":"2021-09-06T14:31:04.645791Z","shell.execute_reply.started":"2021-09-06T14:31:00.788884Z","shell.execute_reply":"2021-09-06T14:31:04.644837Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\n# features.head().T","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-06T14:31:04.647447Z","iopub.execute_input":"2021-09-06T14:31:04.647803Z","iopub.status.idle":"2021-09-06T14:31:04.685512Z","shell.execute_reply.started":"2021-09-06T14:31:04.647763Z","shell.execute_reply":"2021-09-06T14:31:04.684467Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# identify columns\nnumerical_cols = features.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols = features.select_dtypes(include=['object', 'bool']).columns\n\n# useful for column transformers \nnumerical_ix = features.columns.get_indexer(numerical_cols)\ncategorical_ix = features.columns.get_indexer(categorical_cols)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.686933Z","iopub.execute_input":"2021-09-06T14:31:04.687255Z","iopub.status.idle":"2021-09-06T14:31:04.748640Z","shell.execute_reply.started":"2021-09-06T14:31:04.687225Z","shell.execute_reply":"2021-09-06T14:31:04.747524Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# work on a copy\nX_train = features.copy()\nX_test = test.copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.749892Z","iopub.execute_input":"2021-09-06T14:31:04.750282Z","iopub.status.idle":"2021-09-06T14:31:04.836468Z","shell.execute_reply.started":"2021-09-06T14:31:04.750242Z","shell.execute_reply":"2021-09-06T14:31:04.835449Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Helper functions\n\nThese are two functions to save and load predictions, they can be wrapped within a class for a better modeling or kept as they are since they are independent of the project setting.\n","metadata":{}},{"cell_type":"code","source":"\n## helper fucntions\ndef to_file(data, output_folder, idxs=None, suffix='.csv'):\n    print(data)\n    df = pd.DataFrame(data)\n    df.to_csv(f'{output_folder}{os.sep}{suffix}', index=True)\n        \n    \ndef calc_folds_indexes(X, y, n_folds=5, shuffle=True, sampler=KFold, seeds=[42]):\n    \"\"\"\n    Create folds from a dataset X and a target y\n    sampler: can be KFold,  StratifiedKFold, or any sampling class  \n    \n    return a list of dictionaries of {'seed':, 'idxs':[train_idxs, test_idxs]}\n    \"\"\"\n    folds_idxs_list = []\n    for seed in seeds:\n        folds = sampler(n_splits=n_folds, \n                        random_state=seed,\n                        shuffle=shuffle)\n\n        folds_idxs_list.append({'seed': seed, 'idxs':list(folds.split(X, y))})\n        \n    return folds_idxs_list\n        \n\n\n# score function\nscore_func = mean_squared_error\nscore_func_param = {'square':False}\n\ndef score(y, target, average=False):\n    # if y is a list then it will return a list of scores\n    # if average is True then it will return the mean of the scores\n    \n    if type(y) in [list, np.ndarray]:\n        scores = []\n        for y_i in y:\n            scores.append(score_func(y_i, target, **score_func_param))\n        if average:\n            return np.mean(scores)\n        else:\n            return scores\n        \n    return score_func(y, target, **score_func_param)\n    ","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2021-09-06T14:31:04.837829Z","iopub.execute_input":"2021-09-06T14:31:04.838244Z","iopub.status.idle":"2021-09-06T14:31:04.848217Z","shell.execute_reply.started":"2021-09-06T14:31:04.838203Z","shell.execute_reply":"2021-09-06T14:31:04.847272Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### ModelWrapper \nThis class role is to avoid coding multiple classes for each model (or model types). We can see that models can actually be categorized into different categories, where some models accept more parameters than the others. For instance xgboost can use an evaluation set to determine the stopping round number, while Lasso does not accept such extra parameters.\n\nThanks to the flexibility of python and the design of the base models, we can wrap the model and `wrapper` to do what the model should do. In fact, we can easily stretch this class to support sklearn pipelines.\n","metadata":{}},{"cell_type":"code","source":"class ModelWrapper():\n    def __init__(self, \n                 model,\n                 name,\n                 uses_eval_set=False,\n                 fit_params={}):\n        \n        self.model = model\n        self.name = name\n        \n        self.uses_eval_set = uses_eval_set\n        self.fit_params = fit_params # any extra params for the 'fit' function\n                \n    def fit(self, X, y, eval_set=None):\n        if self.uses_eval_set:\n            self.model.fit(X, y, eval_set=eval_set, **(self.fit_params))\n        else:\n            self.model.fit(X, y, **(self.fit_params)) \n        return self\n    \n\n    def predict(self, X):\n        return self.model.predict(X)\n        \n    def clone_me(self, random_state=None):\n        wrapper = ModelWrapper(model=clone(self.model), # clone from sklean.base\n                               name=self.name, \n                               uses_eval_set=self.uses_eval_set,\n                               fit_params=self.fit_params)\n        wrapper.name = self.name\n        if random_state is not None:\n            wrapper.set_random_state(random_state)\n        \n        return wrapper\n    \n    def set_random_state(self, random_state):\n        if hasattr(self.model, 'random_state'):\n            self.model.random_state = random_state\n        elif hasattr(self.model, 'random_seed'):\n            self.model.random_seed = random_state\n            \n    def get_random_state(self):\n        if hasattr(self.model, 'random_state'):\n            return self.model.random_state \n        elif hasattr(self.model, 'random_seed'):\n            return self.model.random_seed\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.849786Z","iopub.execute_input":"2021-09-06T14:31:04.850233Z","iopub.status.idle":"2021-09-06T14:31:04.859820Z","shell.execute_reply.started":"2021-09-06T14:31:04.850193Z","shell.execute_reply":"2021-09-06T14:31:04.858874Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### ModelTrainer\nThis class role is to train a model and calculate the oofs and the test predictions (meta-features).\n","metadata":{}},{"cell_type":"code","source":"class ModelTrainer():\n    def __init__(self,\n                  model: ModelWrapper):\n        \n        self.model = model\n        \n    def calc_oofs(self,\n                  X, y,\n                  X_test,\n                  folds_idxs,\n                  transformer=None,\n                  verbose=False,\n                  use_different_random_states=True):\n        \"\"\"\n        Return the oofs predictions and the meta features (test predictions)\n        \"\"\"\n        \n        test_predictions = 0\n        oof_predictions = np.zeros_like(np.array(y))\n        valid_mean_score = [] \n        for fold, (train_ix, valid_ix) in enumerate(folds_idxs):\n            X_train, X_valid = X[train_ix], X[valid_ix]\n            y_train, y_valid = y[train_ix], y[valid_ix]\n                        \n            \n            # transform input\n            if transformer is not None:\n                X_train = transformer.fit_transform(X_train)\n                X_valid = transformer.transform(X_valid)\n\n            \n            # check if we train each fold on differently initialized clone\n            if use_different_random_states:\n                model = self.model.clone_me(random_state=fold)\n            else:\n                model = self.model.clone_me()\n            \n            # fit the model\n            if model.uses_eval_set:\n                model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)])\n            else:\n                model.fit(X_train, y_train)\n                \n            ## predictions\n            # on the validation set\n            valid_predications = model.predict(X_valid)\n            score = mean_squared_error(valid_predications, y_valid, squared=False)\n            valid_mean_score.append(score)\n            oof_predictions[valid_ix] = valid_predications\n            \n            # on the test set\n            # transform it first on a copy\n            if transformer is not None:\n                X_test_ = transformer.transform(X_test)\n            else:\n                X_test_ = X_test\n                \n            test_predictions += model.predict(X_test_) / len(folds_idxs)\n            \n            if verbose:\n                print('Fold:{} score:{:.4f}'.format(fold + 1, score))\n        \n        if verbose:\n            print('Average score:{:.4f} ({:.4f})'.format(np.mean(valid_mean_score), np.std(valid_mean_score) ))\n    \n        return oof_predictions, test_predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.863328Z","iopub.execute_input":"2021-09-06T14:31:04.863593Z","iopub.status.idle":"2021-09-06T14:31:04.874838Z","shell.execute_reply.started":"2021-09-06T14:31:04.863568Z","shell.execute_reply":"2021-09-06T14:31:04.874015Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Level\nThe level class glues all components in a given layer","metadata":{}},{"cell_type":"code","source":"class Level():\n    def __init__(self,\n                level_id,\n                models,\n                folder,\n                transformer,\n                n_folds=5,\n                seeds=[42],\n                frozen=False,\n                use_different_random_states=True):\n        \n        self.level_id = level_id\n        self.models = models\n        self.folder = folder\n        self.transformer = transformer\n        self.n_folds = n_folds\n        self.seeds = seeds\n        self.frozen = frozen=False\n        self.use_different_random_states = use_different_random_states\n    \n    def create(self, model_zoo):\n        \"\"\"\n         Create a level.\n         model_zoo: a dictionay of all avialable models.\n        \"\"\"\n        self.model_wrappers = []\n        \n        # get models \n        # if models is set to 'all' use all models\n        if self.models[0].lower() == 'all':\n            level_models_names = model_zoo.keys()\n        else: \n            level_models_names = self.models\n\n        for model_name in level_models_names:\n            # get paramaters  \n            model = model_zoo[model_name]\n            fit_kwargs = model_zoo[model_name]['fit_kwargs']\n            app_params = model_zoo[model_name]['app_params']\n\n            model_wrapper = ModelWrapper(model=model['model'], name=model_name)\n            if fit_kwargs is not None:\n                model_wrapper.fit_params = fit_kwargs\n            if app_params is not None:\n                model_wrapper.uses_eval_set = app_params['uses_eval_set']\n            self.model_wrappers.append(model_wrapper)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.876941Z","iopub.execute_input":"2021-09-06T14:31:04.877366Z","iopub.status.idle":"2021-09-06T14:31:04.889538Z","shell.execute_reply.started":"2021-09-06T14:31:04.877326Z","shell.execute_reply":"2021-09-06T14:31:04.888695Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"#### Level Trainer\nTrains all models in a level ","metadata":{}},{"cell_type":"code","source":"class LevelTrainer():\n    def __init__(self,\n                level,\n                seeds_folds_idxs_list):\n        self.level = level\n        self.seeds_folds_idxs_list = seeds_folds_idxs_list\n        \n    def train(self, X_train, y, X_test, verbose=True, agg_func=None):\n        \"\"\"\n        train the level and return the oofs and meta-features for each model in the level\n        if the level has many seeds it will either use the agg_func to combine predictions\n        or will just return eveything\n        \n        agg_func: can be None, np.mean, or any other numpy reduction function\n        \"\"\"\n\n        level_oof_preds, level_test_preds = {}, {}\n        for model_wrapper in self.level.model_wrappers:\n            if verbose:\n                print('-'*30)\n                print(f'Model:{model_wrapper.name}')\n                print('-'*30)\n\n            # train each model with as many times as the length of folds_idxs_list \n            model_oof_preds, model_test_preds = [], []\n            \n            for seeds_folds_idxs in self.seeds_folds_idxs_list:\n                seed, folds_idxs = seeds_folds_idxs['seed'], seeds_folds_idxs['idxs']\n                print('-'*30)\n                print(f'Seed:{seed}')\n                print('-'*30)\n                \n                trainer = ModelTrainer(model_wrapper)\n                oof_preds, test_preds = trainer.calc_oofs(X_train, \n                                                          y,\n                                                          X_test,\n                                                          transformer=self.level.transformer,\n                                                          folds_idxs=folds_idxs,\n                                                          verbose=verbose)\n                if agg_func is None:\n                    level_oof_preds[f'{model_wrapper.name}_seed_{seed}'] =  oof_preds\n                    level_test_preds[f'{model_wrapper.name}_seed_{seed}'] =  test_preds\n                else: # collect them in order to aggregate them with the agg_func function\n                    model_oof_preds.append(oof_preds)\n                    model_test_preds.append(test_preds)\n\n          # aggregate the results\n        if agg_func is not None:\n            level_oof_preds[f'{model_wrapper.name}'] = agg_func(np.column_stack(model_oof_preds))\n            level_test_preds[f'{model_wrapper.name}'] = agg_func(np.column_stack(model_test_preds))\n\n        if verbose:\n            print('-'*30)\n\n        return pd.DataFrame(level_oof_preds), pd.DataFrame(level_test_preds)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.890793Z","iopub.execute_input":"2021-09-06T14:31:04.891085Z","iopub.status.idle":"2021-09-06T14:31:04.906196Z","shell.execute_reply.started":"2021-09-06T14:31:04.891057Z","shell.execute_reply":"2021-09-06T14:31:04.905103Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Experiment \nSince everything boiled down to stacking (even if we have a single level), the experiment class will handle the organization of the resulted files from the test: test and oofs predictions. Therefore, assuming the project has the following structure with a folder called **experiments** we can save our tests in this folder. This is what this class will do. This class is the entry point for any run in the project. It reads the input and the settings and produces the output.\n\n```\n    ML30_project\n    │   README.md\n    │\n    └───notebooks\n    │   ...\n    │\n    └───experiments\n    │   │   \n    │   │\n    │   └───experiment_1   \n    │   │   level_1_oofs.csv\n    │   │   level_1_test.csv\n    │   │   level_2_oofs.csv\n    │   │   level_2_test.csv\n    │   │   ...\n    │   │   meta_level_oofs.csv\n    │   │   meta_level_test.csv\n    │   └───experiment_...\n```\n\n\n>The code that generated the results is important to save too, but that can be done easily by creating a new version of the notebook or copying notebook with the CV_LB results.\n\n>This class is so important when running notebooks in our computers since Kaggle has a nice notebook management system which saves outputs as well.\n\n","metadata":{}},{"cell_type":"code","source":"class Experiment():\n    def __init__(self,\n                 title,\n                 description,\n                 stack,\n                 model_zoo,\n                 main_folder=os.getcwd()):\n        \n        self.title = title\n        self.main_folder = main_folder\n        self.stack = stack\n        self.model_zoo = model_zoo\n        self.description = description\n        # create the main folder if it does not exist\n        if not os.path.exists(f'{self.main_folder}'):\n            os.makedirs(f'{self.main_folder}', exist_ok=True)\n        \n    def join_folder(self, folder=None):\n         \"\"\"\n         Join a folder and output where results will be saved.\n         If 'folder' is None, it will create a folder\n         with a time stamp.\n         \"\"\"\n\n         # create time stamp and subfolder with the current time stamp\n         if folder is not None: # if folder is specified\n            self.output_folder = folder \n            # create a folder if does not exit.\n            folder_path = f'{self.main_folder}{os.sep}{self.output_folder}'\n            if not os.path.exists(folder_path):\n                os.makedirs(folder_path)                \n         else: # create a folder with the time stamp\n            time_stamp = datetime.now().isoformat(' ', 'seconds')\n            self.output_folder = self.title + ' ' + time_stamp.replace(':', '-')\n            # create and replace if it exits.\n            Path(f'{self.main_folder}{os.sep}{self.output_folder}').mkdir(parents=True, exist_ok=True)\n    \n    \n    def run(self, X_train, y, X_test, \n            train_idxs,\n            test_idxs,\n            verbose=True, store=True):\n        \n        # run the stack\n        for  level_params in self.stack:\n            # create all models in the level\n            level = Level(**level_params)\n            level.create(self.model_zoo)\n\n            print('-'*50)\n            print(f'Current Level: {level.level_id}')\n            print('-'*50)\n\n            # join the level's output folder\n            #self.join_folder(folder=level.folder)\n\n            # create folds indexes for the level\n            seeds_folds_idxs_list = calc_folds_indexes(X=X_train,\n                                                       y=y,\n                                                       n_folds=level.n_folds,\n                                                       sampler=KFold,\n                                                       seeds=level.seeds)\n\n            # train the level\n            if not level.frozen:  # escape any trained level   \n                level_trainer = LevelTrainer(level=level, \n                                             seeds_folds_idxs_list=seeds_folds_idxs_list)\n\n                level_oof_preds, level_test_preds =  level_trainer.train(X_train=X_train,\n                                                                         y=y,\n                                                                         X_test=X_test)\n                # store predictions?\n                if store:\n                    # oofs \n                    level_oof_preds.to_csv(f'{self.main_folder}{os.sep}{self.output_folder}{os.sep}{level.level_id}_oofs.csv')\n                    # test predictions\n                    level_test_preds.to_csv(f'{self.main_folder}{os.sep}{self.output_folder}{os.sep}{level.level_id}_test.csv')\n                    \n                \n                # update train and test \n                X_train, X_test = level_oof_preds.values, level_test_preds.values\n            else:\n                print('This level is already trained')\n                # load saved of this level and raise error\n                fold_id = level.n_folds\n                folder =f'{self.main_folder}{os.sep}{self.output_folder}'\n                \n                # new features \n                level_oof_preds = pd.read_csv(f\"{folder}{os.sep}*{fold_id}_oofs.csv\")\n                level_test_preds = pdf.read_csv(f\"{folder}{os.sep}*{fold_id}_test.csv\")\n                \n                X_train = level_oof_preds.values\n                X_test = level_test_preds.values\n                \n            if verbose:\n                display(level_oof_preds.head(10))\n                display(level_test_preds.head(10))\n                \n        # return the last output from the last level\n        return level_test_preds","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.907472Z","iopub.execute_input":"2021-09-06T14:31:04.907745Z","iopub.status.idle":"2021-09-06T14:31:04.924225Z","shell.execute_reply.started":"2021-09-06T14:31:04.907718Z","shell.execute_reply":"2021-09-06T14:31:04.923255Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters\n\nHere goes the paramaters of each model. These can actually be stored in an external JSON file.\n","metadata":{}},{"cell_type":"code","source":"# Lasso\nlasso_params = {\n                'alpha': 0.00005\n}\n\n\n# Elastic Net\nenet_params = {\n               'alpha': 0.00005, \n               'l1_ratio': .9\n}\n\n# extra-tree\net_params = {\n    'n_jobs': -1,\n    'n_estimators': 100,\n    'max_features': 0.5,\n    'max_depth': 12,\n    'min_samples_leaf': 2,\n}\n\n# random forest\n\nrf_params_2 = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n    'max_depth': 5\n}\n\n\nrf_params_1 = {\n    'n_jobs': -1,\n    'n_estimators': 100,\n    'max_features': 0.2,\n    'max_depth': 8,\n    'min_samples_leaf': 2\n}\n\n# gradient boosting \ngb_params = {\n    'n_estimators': 500,\n     'max_depth': 3\n}\n\n# xgboost\n\n# Using optuna but still rough\nxgb_params_2 = {\n                 # gpu (if gpu uncommnet the following three lines)\n#                  'tree_method': 'gpu_hist', \n#                  'gpu_id': 0, \n#                  'predictor': 'gpu_predictor',\n                # cpu\n                 'max_depth': 5,\n                 'learning_rate': 0.021252439960137114,\n                 'n_estimators': 13500,\n                 'subsample': 0.62,\n                 'booster': 'gbtree',\n                 'colsample_bytree': 0.1,\n                 'reg_lambda': 0.1584605320779582,\n                 'reg_alpha': 15.715145781076245,\n                 'n_jobs': -1\n}\n\nxgb_params_1 = {\n            'random_state': 1, \n            # gpu (if gpu uncommnet the following three lines)\n#             'tree_method': 'gpu_hist', \n#             'gpu_id': 0, \n#             'predictor': 'gpu_predictor',\n            # cpu\n            'n_jobs': -1,\n            'booster': 'gbtree',\n            'n_estimators': 10000,\n            # optimized params\n            'learning_rate': 0.03628302216953097,\n            'reg_lambda': 0.0008746338866473539,\n            'reg_alpha': 23.13181079976304,\n            'subsample': 0.7875490025178415,\n            'colsample_bytree': 0.11807135201147481,\n            'max_depth': 3,\n            #'min_child_weight': 6\n}\n\n\n# catboost\ncatb_params = {\n              # gpu (if gpu uncommnet the following two lines)\n#               'task_type': \"GPU\",\n#               'devices': '0:1',\n              # cpu only\n              'iterations': 6800,\n              'learning_rate': 0.93,\n              'loss_function': \"RMSE\",\n              'random_state': 42,\n              'verbose': 0,\n              'thread_count': -1,\n              'depth': 1,\n              'l2_leaf_reg': 3.28}\n\n\n# using optuna\nparams_lgb = {\n            # gpu (if gpu uncommnet the following three lines)\n#             'device' : 'gpu',\n#             'gpu_platform_id':  0,\n#             'gpu_device_id: 0,\n             # cpu only\n             \"n_estimators\": 10000,\n             'metric':'rmse',\n             \"objective\": \"regression\",\n             'max_depth': 12, \n             'subsample': 0.587082286344555, \n             'colsample_bytree': 0.2157299997089329, \n             'learning_rate': 0.01270518267668901,\n             'reg_lambda': 36.78473508062132,\n             'reg_alpha': 14.155146595119032, \n             'min_child_samples': 6, \n             'num_leaves': 34, \n             'max_bin': 914,\n             'cat_smooth': 26,\n             'n_jobs': -1,\n             'cat_l2': 0.020257336654989123\n        }","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.925533Z","iopub.execute_input":"2021-09-06T14:31:04.925813Z","iopub.status.idle":"2021-09-06T14:31:04.942585Z","shell.execute_reply.started":"2021-09-06T14:31:04.925773Z","shell.execute_reply":"2021-09-06T14:31:04.941549Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### These are model/task dependent parameters","metadata":{}},{"cell_type":"code","source":"# external hyperparamaters\n\n### fit function hyperparamaters\n# some models require special paramaters like early stoping in xgboost and lgbm\nfit_params = {'early_stopping_rounds': 300,\n                  'verbose': False}\n\n### application/implementation paramaters\n# These paramaters are implementation dependent \napp_params = {'uses_eval_set':True}\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.944116Z","iopub.execute_input":"2021-09-06T14:31:04.944586Z","iopub.status.idle":"2021-09-06T14:31:04.957157Z","shell.execute_reply.started":"2021-09-06T14:31:04.944541Z","shell.execute_reply":"2021-09-06T14:31:04.956116Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Models","metadata":{}},{"cell_type":"code","source":"lrg = LinearRegression()\n# lasso\nlasso = Lasso(**lasso_params)\n\n# Elastic net\ne_net = ElasticNet(**enet_params)\n\n# KNeighborsRegressor\nknn =  KNeighborsRegressor()\n\n# extra-tree\nextree = ExtraTreesRegressor(**et_params)\n\n# random forest\nrfr = RandomForestRegressor(**rf_params_2)\n\n# gradient boosting\ngb = GradientBoostingRegressor(**gb_params)\n\n#lgbm\nlgb = LGBMRegressor(**params_lgb)\n\n# xgboost \n# variants\nxgb_1 =  XGBRegressor(**xgb_params_1)\nxgb_2 =  XGBRegressor(**xgb_params_2)\n\n#catboost\ncatb = CatBoostRegressor(**catb_params)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.958304Z","iopub.execute_input":"2021-09-06T14:31:04.958557Z","iopub.status.idle":"2021-09-06T14:31:04.972984Z","shell.execute_reply.started":"2021-09-06T14:31:04.958533Z","shell.execute_reply":"2021-09-06T14:31:04.972311Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# compile all settings in one dictionary, \n# we can store/load it then to a JSON file\nmodel_zoo = {\n          'LinearRegression': {\"model\":lrg, \"fit_kwargs\":None, \"app_params\": None},\n          'Lasso': {\"model\":lasso, \"fit_kwargs\":None, \"app_params\": None},\n          'ElasticNet': {\"model\": e_net, \"fit_kwargs\":None, \"app_params\": None},\n          'ExtraTreesRegressor': {\"model\": extree, \"fit_kwargs\":None, \"app_params\": None},\n          'RandomForestRegressor': {\"model\": rfr, \"fit_kwargs\":None, \"app_params\": None},\n          'GradientBoostingRegressor': {\"model\": gb, \"fit_kwargs\":None, \"app_params\": None},\n          'XGBRegressor-1': {\"model\": xgb_1, \"fit_kwargs\":fit_params, \"app_params\": app_params},\n          'XGBRegressor-2': {\"model\": xgb_2, \"fit_kwargs\":fit_params, \"app_params\": app_params},\n          'CatBoostRegressor': {\"model\": catb, \"fit_kwargs\": fit_params, \"app_params\": app_params},\n          'LGBMRegressor': {\"model\": lgb, \"fit_kwargs\":fit_params, \"app_params\": app_params}\n          # we can add any number of models here \n        }","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.974118Z","iopub.execute_input":"2021-09-06T14:31:04.974498Z","iopub.status.idle":"2021-09-06T14:31:04.983843Z","shell.execute_reply.started":"2021-09-06T14:31:04.974460Z","shell.execute_reply":"2021-09-06T14:31:04.982836Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model_zoo.keys()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.984789Z","iopub.execute_input":"2021-09-06T14:31:04.985048Z","iopub.status.idle":"2021-09-06T14:31:04.997703Z","shell.execute_reply.started":"2021-09-06T14:31:04.985015Z","shell.execute_reply":"2021-09-06T14:31:04.996687Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"dict_keys(['LinearRegression', 'Lasso', 'ElasticNet', 'ExtraTreesRegressor', 'RandomForestRegressor', 'GradientBoostingRegressor', 'XGBRegressor-1', 'XGBRegressor-2', 'CatBoostRegressor', 'LGBMRegressor'])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Stacking\n\nHere goes the actual stacking procedure. \n   - We first define the architecture, and setup the a session.\n   - Define the stack. That is, the models and transformers in the levels","metadata":{}},{"cell_type":"code","source":"# settings: experiment and stacking architecutre\n\n# initialize the stack to the input\nX_train_, X_test_ = X_train.copy(), X_test.copy()\n\n# any special transformers for any level\nlevel_1_transformers = [('cat', OrdinalEncoder(), categorical_ix) , ('num', MinMaxScaler(), numerical_ix)]\n#\nlevel_1_transform = ColumnTransformer(transformers=level_1_transformers)\n\n\n# define the actual stack\nstack = [ {\"level_id\": \"level-1\", \n           \"models\": [\n                     #'CatBoostRegressor',\n                     #'XGBRegressor-2',\n                     'XGBRegressor-1',\n                     #'LGBMRegressor'\n                     # we can add any model here\n                    ],\n            \"n_folds\": 5,\n            \"seeds\" : [42, 43, 44 45, 46, 47, 48, 49, 50, 51, 52, 53 ],\n            \"folder\": \"level_1\", \n            \"transformer\": level_1_transform, \n            \"frozen\": True # to freeze the level if already trained\n            },\n               \n#            {\"level_id\": \"level-2\",\n#             \"models\": [\n#                        'RandomForestRegressor',\n#                        #'CatBoostRegressor',\n#                      #  other models can be added here\n#                       ],\n#             \"n_folds\": 10,\n#             \"seeds\" : [43, 45, 47, 49],\n#             \"folder\": \"level_2\",\n#             \"transformer\": None,\n#             \"frozen\": False\n#           },\n         \n         # we can add any number of levels here\n         # ...\n         \n          {\"level_id\": \"meta_level\",\n            \"models\": [#'LinearRegression',\n                       'RandomForestRegressor'\n                      ],\n            \"n_folds\": 5,\n            \"seeds\" : [42],\n            \"folder\": \"meta_level\",\n            \"transformer\": None,\n            \"frozen\": False\n          }\n         \n         \n        ]\n         ","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:04.999014Z","iopub.execute_input":"2021-09-06T14:31:04.999556Z","iopub.status.idle":"2021-09-06T14:31:05.009755Z","shell.execute_reply.started":"2021-09-06T14:31:04.999524Z","shell.execute_reply":"2021-09-06T14:31:05.008372Z"},"trusted":true},"execution_count":18,"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-b9ceab68bfae>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    \"seeds\" : [42, 43, 44 45, 46, 47, 48, 49, 50, 51, 52, 53 ],\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-18-b9ceab68bfae>, line 22)","output_type":"error"}]},{"cell_type":"markdown","source":"- Loop through each level in the stack","metadata":{}},{"cell_type":"code","source":"# create experiment\nexperiments_folder = \"Experiments\"\nexperiment_folder = 'experiement_1' # if None a folder with time stamp will be created\nexperiment_description = \"Simple model, multiple seeds\"\n\n\nml30_experiment = Experiment(title='ML 30 days',\n                             description=experiment_description,\n                             stack=stack,\n                             model_zoo=model_zoo,\n                             main_folder=f'{os.getcwd()}{os.sep}{experiments_folder}')\n\nml30_experiment.join_folder(experiment_folder)\n\nresults = ml30_experiment.run(X_train=X_train_.values,\n                     y=y.values, \n                     X_test=X_test_.values,\n                     train_idxs = X_train_.index,\n                     test_idxs = X_test_.index)\n\n","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2021-09-06T14:31:05.010872Z","iopub.status.idle":"2021-09-06T14:31:05.011665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final results\nresults.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:05.012921Z","iopub.status.idle":"2021-09-06T14:31:05.013685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submit the results","metadata":{}},{"cell_type":"code","source":"predictions = results.iloc[:, -1].values","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:05.014913Z","iopub.status.idle":"2021-09-06T14:31:05.015668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the predictions to a CSV file\noutput = pd.DataFrame({'id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:05.016859Z","iopub.status.idle":"2021-09-06T14:31:05.017619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results \noutput.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:31:05.018772Z","iopub.status.idle":"2021-09-06T14:31:05.019383Z"},"trusted":true},"execution_count":null,"outputs":[]}]}