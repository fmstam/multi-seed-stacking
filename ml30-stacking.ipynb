{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This notebook describes how to structure the project.\n\nDuring the competition there were many tasks to do, plan, research, test, tune, accept disappointment of failed tests, and be happy with a little improvement. \n\nTherefore, the workspace and the files of the project should be structured in a flexible way with less repeated code. In other word, to split code from data/configuration to save time and reduce errors/bugs.\n\nTherefore, let us first we define the entities in the project.\nThere are four main entities and I think these will be the same in all projects: experiment, model, level, and stack. These will be modeled by classes as described in this notebook.\n\n\nI would like to give credits to many kernels and websites among them:\n\n - Good introduction introduction about stacking: https://mlwave.com/kaggle-ensembling-guide/\n - Implementation of stacking and some nice discussion https://www.kaggle.com/getting-started/18153#post103381\n - Stacking solution for a regression problem https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n \n*This is a draft work, and will be improved regularly.*\n","metadata":{}},{"cell_type":"code","source":"# Familiar imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\nimport os\nimport glob\nfrom datetime import datetime\nfrom pathlib import Path\n\n\n\n\n# helpers\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder, PowerTransformer, StandardScaler, \\\n                                  MinMaxScaler, RobustScaler, PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\nfrom sklearn.pipeline import make_pipeline\n\n# Models\nfrom sklearn.linear_model import ElasticNet, Lasso, LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# base\nfrom sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, clone\n\n# scoring\nfrom sklearn.metrics import mean_squared_error\n","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:29:54.592433Z","iopub.execute_input":"2021-09-03T08:29:54.593205Z","iopub.status.idle":"2021-09-03T08:29:57.446051Z","shell.execute_reply.started":"2021-09-03T08:29:54.593061Z","shell.execute_reply":"2021-09-03T08:29:57.445168Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"# notebook options\npd.set_option(\"display.max_columns\", 100)\npath = \"../input/30-days-of-ml/\"\ntrain_file = \"train.csv\"\ntest_file = \"test.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:29:57.447341Z","iopub.execute_input":"2021-09-03T08:29:57.447614Z","iopub.status.idle":"2021-09-03T08:29:57.453660Z","shell.execute_reply.started":"2021-09-03T08:29:57.447588Z","shell.execute_reply":"2021-09-03T08:29:57.452774Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load the training data\ntrain = pd.read_csv(f'{path}{os.sep}{train_file}', index_col=0)\ntest = pd.read_csv(f'{path}{os.sep}{test_file}', index_col=0)\n\n# Preview the data\n# train.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:29:57.455070Z","iopub.execute_input":"2021-09-03T08:29:57.455352Z","iopub.status.idle":"2021-09-03T08:30:08.698885Z","shell.execute_reply.started":"2021-09-03T08:29:57.455326Z","shell.execute_reply":"2021-09-03T08:30:08.697726Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\n# features.head().T","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-03T08:30:08.700759Z","iopub.execute_input":"2021-09-03T08:30:08.701198Z","iopub.status.idle":"2021-09-03T08:30:08.748665Z","shell.execute_reply.started":"2021-09-03T08:30:08.701154Z","shell.execute_reply":"2021-09-03T08:30:08.747346Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# identify columns\nnumerical_cols = features.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols = features.select_dtypes(include=['object', 'bool']).columns\n\n# useful for column transformers \nnumerical_ix = features.columns.get_indexer(numerical_cols)\ncategorical_ix = features.columns.get_indexer(categorical_cols)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:08.750694Z","iopub.execute_input":"2021-09-03T08:30:08.751203Z","iopub.status.idle":"2021-09-03T08:30:08.816925Z","shell.execute_reply.started":"2021-09-03T08:30:08.751152Z","shell.execute_reply":"2021-09-03T08:30:08.815766Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# work on a copy\nX_train = features.copy()\nX_test = test.copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:08.818195Z","iopub.execute_input":"2021-09-03T08:30:08.818497Z","iopub.status.idle":"2021-09-03T08:30:08.903888Z","shell.execute_reply.started":"2021-09-03T08:30:08.818469Z","shell.execute_reply":"2021-09-03T08:30:08.903010Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Helper functions\n\nThese are two functions to save and load predictions, they can be wrapped within a class for a better modeling or kept as they are since they are independent of the project setting.\n","metadata":{}},{"cell_type":"code","source":"\n## helper fucntions\ndef store_oof_predictions(model_name,\n                          valid_predictions,\n                          test_predictions,\n                          X_train,\n                          X_test,\n                          output_folder,\n                          n_folds, \n                          random_state):    \n    \n    \"\"\"\n    Store oof predictions into files.\n    It is safe not to use defaults for this helper to make sure \n    we are storing the right experiement. \n    \"\"\"\n    \n    if test_predictions is not None:\n        test_df = pd.DataFrame({'Id': X_test.index,\n                       model_name: test_predictions})\n        test_df.to_csv( f'{output_folder}{os.sep}{model_name}_{n_folds}_test.csv',\n                       index=False)\n\n    if valid_predictions is not None:\n        valid_df = pd.DataFrame({'Id': X_train.index,\n                           model_name: valid_predictions})\n        valid_df.to_csv(f'{output_folder}{os.sep}{model_name}_{n_folds}_valid.csv', \n                        index=False)\n    \n    \ndef read_oof_predictions(pattern, \n                     models_names, \n                     index='id'):    \n    \"\"\"\n    Read files according the pattern in their name\n    it returns a dataframe where each column is the prediction of a model\n    \"\"\"\n    li = [] \n    for f in glob.glob(pattern):\n        file_name = f.split(os.sep)[-1] # get file name only\n        if file_name.lower().startswith(tuple(s.lower() for s in models_names)): # get only required models\n            df = pd.read_csv(f, index_col=0)\n            li.append(df)\n    \n                                   \n    return pd.concat(li, axis=1)\n    ","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2021-09-03T08:30:08.904832Z","iopub.execute_input":"2021-09-03T08:30:08.905131Z","iopub.status.idle":"2021-09-03T08:30:08.915249Z","shell.execute_reply.started":"2021-09-03T08:30:08.905085Z","shell.execute_reply":"2021-09-03T08:30:08.914170Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Experiment \nSince everything boiled down to stacking (even if we have a single level), the experiment class will handle the organization of the resulted files from the test: test and oofs predictions. Therefore, assuming the project has the following structure with a folder called **experiments** we can save our tests in this folder. This is what this class will do. Another important thing is to create the CV folds, and use them. Currently the class supports one seed only, but that can be changed easily by creating a list of CV indexes instead of a single one. I will add that in the next round.\n\n```\n    ML30_project\n    │   README.md\n    │\n    └───notebooks\n    │   ...\n    │\n    └───experiments\n    │   │   \n    │   │\n    │   └───experiment_1\n    │   │   │   \n    │   │   └───level_1\n    │   │   │   xgb_test.csv\n    │   │   │   xgb_test.csv\n    │   │   │   ...\n    │   │   └───level_2\n    │   │   │   ...\n    │   │   └───level_3\n    │   │   │   ...\n    │   │   └───level_...\n    │   │\n    │   │\n    │   └───experiment_...\n```\n\n\n>The code that generated the results is important to save too, but that can be done easily by creating a new version of the notebook or copying notebook with the CV_LB results.\n\n>This class is so important when running notebooks in our computers since Kaggle has a nice notebook management system which saves outputs as well.\n\n","metadata":{}},{"cell_type":"code","source":"\nclass Experiment():\n    def __init__(self,\n                 title='Experiment',\n                 n_folds=3,\n                 random_state=42,\n                 shuffle=True,\n                 use_different_random_states=True,\n                 main_folder=os.getcwd()):\n        \n        self.title = title\n        self.n_folds = n_folds\n        self.random_state = random_state\n        self.shuffle = shuffle\n        self.use_different_random_states = use_different_random_states\n        self.main_folder = main_folder\n        \n        # create the main folder if it does not exist\n        if not os.path.exists(f'{self.main_folder}'):\n            os.makedirs(f'{self.main_folder}')\n        \n    def calc_folds_indexes(self, X, y, n_folds=None, sampler=KFold):\n        \"\"\"\n        Create folds from a dataset X and a target y\n        sampler: can be KFold,  StratifiedKFold, or any sampling class  \n        \"\"\"\n        # if no number of folds are specified use the global number\n        if n_folds is None: \n            n_folds = self.n_folds\n            \n        self.folds = sampler(n_splits=self.n_folds, \n                        random_state=self.random_state,\n                        shuffle=self.shuffle)\n        self.folds_idxs = list(self.folds.split(X, y))\n        \n        \n        \n    def join_folder(self, folder=None):\n         \"\"\"\n         Join the current working folder. Therefore, any output will be written to the folder.\n         This is important as we move from one level to another during the training of the stack\n         if folder does not exist, it will create it first. if it is None, it will create a folder\n         with a time stamp.\n         \"\"\"\n\n         # create time stamp and subfolder with the current time stamp\n         if folder is not None: # if folder is specified\n            self.output_folder = folder \n            # create a folder if does not exit.\n            folder_path = f'{self.main_folder}{os.sep}{self.output_folder}'\n            if not os.path.exists(folder_path):\n                os.makedirs(folder_path)                \n         else: # create a folder with the time stamp\n            time_stamp = datetime.now().isoformat(' ', 'seconds')\n            self.output_folder = self.title + ' ' + time_stamp.replace(':', '-')\n            # create and replace if it exits.\n            Path(f'{self.main_folder}{os.sep}{self.output_folder}').mkdir(parents=True, exist_ok=True)\n    \n    def to_dict(self):\n        return dict({\"title\": self.title,\n                    \"n_folds\": self.n_folds,\n                    \"random_state\": self.random_state,\n                    \"use_different_random_states\": self.use_different_random_states,\n                    \"main_folder\": self.main_folder,\n                    \"output_folder\": self.output_folder})\n        \n    def __str__(self):\n        return str(self.to_dict())","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:08.917996Z","iopub.execute_input":"2021-09-03T08:30:08.918364Z","iopub.status.idle":"2021-09-03T08:30:08.932149Z","shell.execute_reply.started":"2021-09-03T08:30:08.918332Z","shell.execute_reply":"2021-09-03T08:30:08.931059Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### ModelWrapper \nThis class role is to avoid coding multiple classes for each model (or model types). We can see that models can actually be categorized into different categories, where some models accept more parameters than the others. For instance xgboost can use an evaluation set to determine the stopping round number, while Lasso does not accept such extra parameters.\n\nThanks to the flexibility of python and the design of the base models, we can wrap the model and `wrapper` to do what the model should do. In fact, we can easily stretch this class to support sklearn pipelines.\n","metadata":{}},{"cell_type":"code","source":"class ModelWrapper():\n    def __init__(self, \n                 model,\n                 name,\n                 uses_eval_set=False,\n                 fit_params={}):\n        \n        self.model = model\n        self.name = name\n        \n        self.uses_eval_set = uses_eval_set\n        self.fit_params = fit_params # any extra params for the 'fit' function\n                \n    def fit(self, X, y, eval_set=None):\n        if self.uses_eval_set:\n            self.model.fit(X, y, eval_set=eval_set, **(self.fit_params))\n        else:\n            self.model.fit(X, y, **(self.fit_params)) \n        return self\n    \n\n    def predict(self, X):\n        return self.model.predict(X)\n        \n    def clone_me(self, random_state=None):\n        wrapper = ModelWrapper(model=clone(self.model), # clone from sklean.base\n                               name=self.name, \n                               uses_eval_set=self.uses_eval_set,\n                               fit_params=self.fit_params)\n        wrapper.name = self.name\n        if random_state is not None:\n            wrapper.set_random_state(random_state)\n        \n        return wrapper\n    \n    def set_random_state(self, random_state):\n        if hasattr(self.model, 'random_state'):\n            self.model.random_state = random_state\n        elif hasattr(self.model, 'random_seed'):\n            self.model.random_seed = random_state\n            \n    def get_random_state(self):\n        if hasattr(self.model, 'random_state'):\n            return self.model.random_state \n        elif hasattr(self.model, 'random_seed'):\n            return self.model.random_seed\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:08.933937Z","iopub.execute_input":"2021-09-03T08:30:08.934293Z","iopub.status.idle":"2021-09-03T08:30:08.949690Z","shell.execute_reply.started":"2021-09-03T08:30:08.934250Z","shell.execute_reply":"2021-09-03T08:30:08.948934Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### ModelTrainer\nThis class role is to the a model and calculate the oofs and the test results.\n","metadata":{}},{"cell_type":"code","source":"class ModelTrainer():\n    def __init__(self,\n                  model: ModelWrapper):\n        \n        self.model = model\n        \n    def calc_oofs(self,\n                  X, y,\n                  X_test,\n                  folds_idxs,\n                  transformer=None,\n                  verbose=False,\n                  use_different_random_states=True):\n        \"\"\"\n        Return the oofs predictions and the meta features (test predictions)\n        \"\"\"\n        \n        test_predictions = 0\n        oof_predictions = np.zeros_like(np.array(y))\n        valid_mean_score = [] \n                \n        for fold, (train_ix, valid_ix) in enumerate(folds_idxs):\n            X_train, X_valid = X[train_ix], X[valid_ix]\n            y_train, y_valid = y[train_ix], y[valid_ix]\n                        \n            \n            # transform input\n            if transformer is not None:\n                X_train = transformer.fit_transform(X_train)\n                X_valid = transformer.transform(X_valid)\n\n            \n            # check if we train each fold on differently initialized clone\n            if use_different_random_states:\n                model = self.model.clone_me(random_state=fold)\n            else:\n                model = self.model.clone_me()\n            \n            # fit the model\n            if model.uses_eval_set:\n                model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)])\n            else:\n                model.fit(X_train, y_train)\n                \n            ## predictions\n            # on the validation set\n            valid_predications = model.predict(X_valid)\n            score = mean_squared_error(valid_predications, y_valid, squared=False)\n            valid_mean_score.append(score)\n            oof_predictions[valid_ix] = valid_predications\n            \n            # on the test set\n            # transform it first on a copy\n            if transformer is not None:\n                X_test_ = transformer.transform(X_test)\n            else:\n                X_test_ = X_test\n            test_predictions += model.predict(X_test_) / len(folds_idxs)\n            \n            if verbose:\n                print('Fold:{} score:{:.4f}'.format(fold + 1, score))\n        \n        if verbose:\n            print('Average score:{:.4f} ({:.4f})'.format(np.mean(valid_mean_score), np.std(valid_mean_score) ))\n    \n        return oof_predictions, test_predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:08.950842Z","iopub.execute_input":"2021-09-03T08:30:08.951148Z","iopub.status.idle":"2021-09-03T08:30:08.968846Z","shell.execute_reply.started":"2021-09-03T08:30:08.951118Z","shell.execute_reply":"2021-09-03T08:30:08.968072Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### LevelTrainer\nThis class role glues, everything in one place and train each model in a given level.\n","metadata":{}},{"cell_type":"code","source":"class LevelTrainer():\n    \n    def __init__(self,\n                 experiment, # Experiment,\n                 models,      # ModelWrapper\n                 transformer, # data transformer\n                 level_num=1):\n        self.level_num = level_num\n        self.experiment = experiment\n        self.models = models\n        self.transformer = transformer\n        \n    def train(self, X_df, y_df, X_test_df, verbose=True):\n        \n        assert models is not None\n        \n\n        for model_wrapper in self.models:\n            \n            if verbose:\n                print('-'*30)\n                print(f'Model:{model_wrapper.name}')\n                print('-'*30)\n                \n            trainer = ModelTrainer(model_wrapper)\n            oof_preds, test_preds = trainer.calc_oofs(X_df.values, y_df.values,\n                                                      X_test_df.values,\n                                                      transformer=self.transformer,\n                                                      folds_idxs=self.experiment.folds_idxs,\n                                                      verbose=verbose,\n                                                      use_different_random_states=self.experiment.use_different_random_states)\n            # store predictions\n            store_oof_predictions(model_name=model_wrapper.name,\n                                  valid_predictions=oof_preds,\n                                  test_predictions=test_preds,\n                                  X_train=X_df,\n                                  X_test=X_test_df,\n                                  n_folds=self.experiment.n_folds, \n                                  random_state=model_wrapper.get_random_state(),\n                                  output_folder=f'{self.experiment.main_folder}{os.sep}{self.experiment.output_folder}')\n            if verbose:\n                print('-'*30)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:08.970250Z","iopub.execute_input":"2021-09-03T08:30:08.970554Z","iopub.status.idle":"2021-09-03T08:30:08.986294Z","shell.execute_reply.started":"2021-09-03T08:30:08.970525Z","shell.execute_reply":"2021-09-03T08:30:08.985355Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters\n\nHere goes the paramaters of each model. These can actually be stored in an external JSON file.\n","metadata":{}},{"cell_type":"code","source":"# Lasso\nlasso_params = {\n                'alpha': 0.00005\n}\n\n\n# Elastic Net\nenet_params = {\n               'alpha': 0.00005, \n               'l1_ratio': .9\n}\n\n# extra-tree\net_params = {\n    'n_jobs': -1,\n    'n_estimators': 100,\n    'max_features': 0.5,\n    'max_depth': 12,\n    'min_samples_leaf': 2,\n}\n\n# random forest\n\nrf_params_2 = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n    'max_depth': 5\n}\n\n\nrf_params_1 = {\n    'n_jobs': -1,\n    'n_estimators': 100,\n    'max_features': 0.2,\n    'max_depth': 8,\n    'min_samples_leaf': 2\n}\n\n# gradient boosting \ngb_params = {\n    'n_estimators': 500,\n     'max_depth': 3\n}\n\n# xgboost\n\n# Using optuna but still rough\nxgb_params_2 = {\n#                  'tree_method': 'gpu_hist', \n#                  'gpu_id': 0, \n#                  'predictor': 'gpu_predictor',\n    \n                 'max_depth': 5,\n                 'learning_rate': 0.021252439960137114,\n                 'n_estimators': 13500,\n                 'subsample': 0.62,\n                 'booster': 'gbtree',\n                 'colsample_bytree': 0.1,\n                 'reg_lambda': 0.1584605320779582,\n                 'reg_alpha': 15.715145781076245,\n                 'n_jobs': -1\n}\n\nxgb_params_1 = {\n            'random_state': 1, \n            # gpu\n    #         'tree_method': 'gpu_hist', \n    #         'gpu_id': 0, \n    #         'predictor': 'gpu_predictor',\n            # cpu\n            'n_jobs': -1,\n            'booster': 'gbtree',\n            'n_estimators': 10000,\n            # optimized params\n            'learning_rate': 0.03628302216953097,\n            'reg_lambda': 0.0008746338866473539,\n            'reg_alpha': 23.13181079976304,\n            'subsample': 0.7875490025178415,\n            'colsample_bytree': 0.11807135201147481,\n            'max_depth': 3,\n            #'min_child_weight': 6\n}\n\n\n# catboost\ncatb_params = {'iterations': 6800,\n              'learning_rate': 0.93,\n              'loss_function': \"RMSE\",\n              'random_state': 42,\n              'verbose': 0,\n              'thread_count': -1,\n              'depth': 1,\n              'l2_leaf_reg': 3.28}\n\n\n# using optuna\nparams_lgb = {\n             \"n_estimators\": 10000,\n             'metric':'rmse',\n             \"objective\": \"regression\",\n             'max_depth': 12, \n             'subsample': 0.587082286344555, \n             'colsample_bytree': 0.2157299997089329, \n             'learning_rate': 0.01270518267668901,\n             'reg_lambda': 36.78473508062132,\n             'reg_alpha': 14.155146595119032, \n             'min_child_samples': 6, \n             'num_leaves': 34, \n             'max_bin': 914,\n             'cat_smooth': 26,\n             'n_jobs': -1,\n             'cat_l2': 0.020257336654989123\n        }","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:08.987438Z","iopub.execute_input":"2021-09-03T08:30:08.987830Z","iopub.status.idle":"2021-09-03T08:30:09.006317Z","shell.execute_reply.started":"2021-09-03T08:30:08.987801Z","shell.execute_reply":"2021-09-03T08:30:09.005388Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### These are model/task dependent parameters","metadata":{}},{"cell_type":"code","source":"# external hyperparamaters\n\n### fit function hyperparamaters\n# some models require special paramaters like early stoping in xgboost and lgbm\nfit_params = {'early_stopping_rounds': 300,\n                  'verbose': False}\n\n### application/implementation paramaters\n# These paramaters are implementation dependent \napp_params = {'uses_eval_set':True}\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:09.007564Z","iopub.execute_input":"2021-09-03T08:30:09.008108Z","iopub.status.idle":"2021-09-03T08:30:09.023660Z","shell.execute_reply.started":"2021-09-03T08:30:09.008065Z","shell.execute_reply":"2021-09-03T08:30:09.022636Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Models\nHere we define our models.","metadata":{}},{"cell_type":"code","source":"lrg = LinearRegression()\n# lasso\nlasso = Lasso(**lasso_params)\n\n# Elastic net\ne_net = ElasticNet(**enet_params)\n\n# KNeighborsRegressor\nknn =  KNeighborsRegressor()\n\n# extra-tree\nextree = ExtraTreesRegressor(**et_params)\n\n# random forest\nrfr = RandomForestRegressor(**rf_params_2)\n\n# gradient boosting\ngb = GradientBoostingRegressor(**gb_params)\n\n#lgbm\nlgb = LGBMRegressor(**params_lgb)\n\n# xgboost \n# variants\nxgb_1 =  XGBRegressor(**xgb_params_1)\nxgb_2 =  XGBRegressor(**xgb_params_2)\n\n#catboost\ncatb = CatBoostRegressor(**catb_params)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:09.024878Z","iopub.execute_input":"2021-09-03T08:30:09.025540Z","iopub.status.idle":"2021-09-03T08:30:09.037830Z","shell.execute_reply.started":"2021-09-03T08:30:09.025505Z","shell.execute_reply":"2021-09-03T08:30:09.036555Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# compile all settings in one dictionary, \n# we can store/load it then to a JSON file\nmodels = {'LinearRegression': {\"model\":lrg, \"fit_kwargs\":None, \"app_params\": None},\n          'Lasso': {\"model\":lasso, \"fit_kwargs\":None, \"app_params\": None},\n          'ElasticNet': {\"model\": e_net, \"fit_kwargs\":None, \"app_params\": None},\n          'ExtraTreesRegressor': {\"model\": extree, \"fit_kwargs\":None, \"app_params\": None},\n          'RandomForestRegressor': {\"model\": rfr, \"fit_kwargs\":None, \"app_params\": None},\n          'GradientBoostingRegressor': {\"model\": gb, \"fit_kwargs\":None, \"app_params\": None},\n          'XGBRegressor-1': {\"model\": xgb_1, \"fit_kwargs\":fit_params, \"app_params\": app_params},\n          'XGBRegressor-2': {\"model\": xgb_2, \"fit_kwargs\":fit_params, \"app_params\": app_params},\n          'CatBoostRegressor': {\"model\": catb, \"fit_kwargs\": fit_params, \"app_params\": app_params},\n          'LGBMRegressor': {\"model\": lgb, \"fit_kwargs\":fit_params, \"app_params\": app_params}\n          # we can add any number of models here \n        }","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:09.039409Z","iopub.execute_input":"2021-09-03T08:30:09.039742Z","iopub.status.idle":"2021-09-03T08:30:09.050428Z","shell.execute_reply.started":"2021-09-03T08:30:09.039711Z","shell.execute_reply":"2021-09-03T08:30:09.049393Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"models.keys()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:09.051700Z","iopub.execute_input":"2021-09-03T08:30:09.052017Z","iopub.status.idle":"2021-09-03T08:30:09.068959Z","shell.execute_reply.started":"2021-09-03T08:30:09.051986Z","shell.execute_reply":"2021-09-03T08:30:09.067941Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"dict_keys(['LinearRegression', 'Lasso', 'ElasticNet', 'ExtraTreesRegressor', 'RandomForestRegressor', 'GradientBoostingRegressor', 'XGBRegressor-1', 'XGBRegressor-2', 'CatBoostRegressor', 'LGBMRegressor'])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Stacking\n\nHere goes the actual stacking procedure. \n   - We first define the architecture, and setup the a session.\n   - Define the stack. That is, the models and transformers in the levels","metadata":{}},{"cell_type":"code","source":"# settings: experiment and stacking architecutre\nsession_folder = \"Experiments/session_1\"\n# create experiment\nml30_experiment = Experiment(title='ML 30 days',\n                    n_folds=5,\n                    random_state=42,\n                    shuffle=True,\n                    use_different_random_states=True,\n                    main_folder=f'{os.getcwd()}{os.sep}{session_folder}')\n\n# initialize the stack to the input\nX_train_, X_test_ = X_train, X_test \n\n# any special transformers for any level\nlevel_1_transformers = [('cat', OrdinalEncoder(), categorical_ix), ('num', MinMaxScaler(), numerical_ix)]\nlevel_1_transform = ColumnTransformer(transformers=level_1_transformers)\n\n\n# define the actual stack\nstack = [ {\"level-id\": \"level-1\", \n           \"models\": [\n                     'CatBoostRegressor',\n                     'XGBRegressor-1',\n                     # we can add any model here\n                    ],\n            \"use-only\": None, # to select subset of models in the next level\n            \"n_folds\": 10,\n            \"folder\": \"level_1\", \n            \"transformer\": level_1_transform, \n            \"frozen\": True # to freeze the level if already trained\n            },\n               \n           {\"level-id\": \"level-2\",\n            \"models\": [\n                       'RandomForestRegressor',\n                       'LinearRegression',\n                     #  other models can be added here\n                      ],\n            \"use-only\": None, \n            \"n_folds\": 10,\n            \"folder\": \"level_2\",\n            \"transformer\": None,\n            \"frozen\": False\n          },\n         \n         # we can add any number of levels here\n         # ...\n         \n          {\"level-id\": \"meta_level\",\n            \"models\": ['RandomForestRegressor'],\n            \"use-only\": None, \n            \"n_folds\": 10,\n            \"folder\": \"meta_level\",\n            \"transformer\": None,\n            \"frozen\": False\n          }\n         \n         \n        ]\n         ","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:09.070323Z","iopub.execute_input":"2021-09-03T08:30:09.070716Z","iopub.status.idle":"2021-09-03T08:30:09.082192Z","shell.execute_reply.started":"2021-09-03T08:30:09.070682Z","shell.execute_reply":"2021-09-03T08:30:09.081074Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"- Loop through each level in the stack","metadata":{}},{"cell_type":"code","source":"# run the stack\nfor  level in stack:\n    \n    print('-'*50)\n    print(f'Current Level: {level[\"level-id\"]}')\n    print('-'*50)\n    \n    # get current path\n    \n    # get models \n    # if level_1_models is set to 'all' use all models\n    if level['models'][0].lower() == 'all':\n        level_models_names = models.keys()\n    else: \n        level_models_names = level['models']\n\n    # join the level's folder\n    ml30_experiment.join_folder(folder=level['folder'])\n    # create folds indexes for the level\n    ml30_experiment.calc_folds_indexes(X=X_train_.values, y=y.values)\n\n    # create models\n    model_wrappers = []\n    for model_name in level_models_names:\n\n        # get paramaters  \n        model = models[model_name]\n        fit_kwargs = models[model_name]['fit_kwargs']\n        app_params = models[model_name]['app_params']\n\n        model_wrapper = ModelWrapper(model=model['model'], name=model_name)\n        if fit_kwargs is not None:\n            model_wrapper.fit_params = fit_kwargs\n        if app_params is not None:\n            model_wrapper.uses_eval_set = app_params['uses_eval_set']\n        model_wrappers.append(model_wrapper)\n\n        \n    # train the level\n    \n    if not level['frozen']:  # escape any trained level   \n        level_trainer = LevelTrainer(experiment=ml30_experiment, \n                             models=model_wrappers, \n                             transformer=level['transformer'])\n\n        level_trainer.train(X_df=X_train_, y_df=y, X_test_df=X_test_)\n    else:\n        print('This level is already trained')\n\n\n    # collect results    \n    fold_id = ml30_experiment.n_folds\n    folder =f'{session_folder}{os.sep}{ml30_experiment.output_folder}'\n                            \n    print(f\"{folder}{os.sep}*{fold_id}_valid.csv\")\n    \n    # new features \n    X_train_ = read_oof_predictions(pattern=f\"{folder}{os.sep}*{fold_id}_valid.csv\",\n                               models_names=level_models_names)\n    X_test_ = read_oof_predictions(pattern=f\"{folder}{os.sep}*{fold_id}_test.csv\",\n                              models_names=level_models_names)\n    \n    # to see each level's output\n#     display(X_train_.head(10))\n#     display(X_test_.head(10))\n","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2021-09-03T08:30:09.084059Z","iopub.execute_input":"2021-09-03T08:30:09.084495Z","iopub.status.idle":"2021-09-03T08:30:09.746688Z","shell.execute_reply.started":"2021-09-03T08:30:09.084453Z","shell.execute_reply":"2021-09-03T08:30:09.744384Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"--------------------------------------------------\nCurrent Level: level-1\n--------------------------------------------------\nThis level is already trained\nExperiments/session_1/level_1/*5_valid.csv\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-8cb55df59ad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# new features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     X_train_ = read_oof_predictions(pattern=f\"{folder}{os.sep}*{fold_id}_valid.csv\",\n\u001b[0;32m---> 59\u001b[0;31m                                models_names=level_models_names)\n\u001b[0m\u001b[1;32m     60\u001b[0m     X_test_ = read_oof_predictions(pattern=f\"{folder}{os.sep}*{fold_id}_test.csv\",\n\u001b[1;32m     61\u001b[0m                               models_names=level_models_names)\n","\u001b[0;32m<ipython-input-7-cf34ace81a11>\u001b[0m in \u001b[0;36mread_oof_predictions\u001b[0;34m(pattern, models_names, index)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     )\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No objects to concatenate"],"ename":"ValueError","evalue":"No objects to concatenate","output_type":"error"}]},{"cell_type":"code","source":"# final results\nX_test_.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:09.747621Z","iopub.status.idle":"2021-09-03T08:30:09.748063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submit the results","metadata":{}},{"cell_type":"code","source":"predictions = X_test_.iloc[:, -1].values","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:09.749312Z","iopub.status.idle":"2021-09-03T08:30:09.749738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:09.750850Z","iopub.status.idle":"2021-09-03T08:30:09.751311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results \noutput.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T08:30:09.752349Z","iopub.status.idle":"2021-09-03T08:30:09.752765Z"},"trusted":true},"execution_count":null,"outputs":[]}]}